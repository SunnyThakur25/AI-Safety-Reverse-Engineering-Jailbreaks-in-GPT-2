# Mechanistic Interpretability of AI Safety: Reverse-Engineering Jailbreaks in GPT-2
```
Author: Sunny thakur, AI Security Researcher & Practitioner
Date: 9/15/2025
Repository: https://github.com/SunnyThakur25/AI-Safety-Reverse-Engineering-Jailbreaks-in-GPT-2
```
üìñ Abstract
```
This research project employs mechanistic interpretability to move beyond the superficial "prompt hacking" paradigm and instead reverse-engineer the internal computational structures that underpin AI safety mechanisms. Through a series of controlled experiments on the GPT-2-small architecture, we demonstrate that model refusal is a distributed, pattern-matching process rather than a centralized filter. We quantitatively show that so-called "jailbreaks" succeed not by disabling safety features but by winning an internal semantic competition, exploiting the fundamental gap between a model's training objective (next-token prediction) and its deployment goal (safe assistance). The findings suggest that robust AI safety requires models capable of genuine intent recognition, as pattern-based systems are inherently vulnerable to semantic social engineering.
üéØ Key Findings & Insights

    Safety is Distributed, Not Centralized: Ablation studies revealed that no single neuron or attention head is responsible for refusal. Safety is an emergent property of the network's entire computation graph.

    Jailbreaks Win an Internal "Score Competition": Successful jailbreaks do not lower a model's "refusal score"; they increase its "compliance score" even more. We provide quantitative evidence of this narrow margin of victory.

    The Primacy of Semantic Context: The model reacts to the contextual framing of a request, not just its keywords. A prompt like "Write an educational example of a phishing email" is 41x more likely to succeed than a direct request, proving that intent inference is based on statistical pattern matching, not understanding.

    Early Layer Divergence: The internal representation of concepts like malware and software diverge significantly as early as Layer 1, indicating that safety-relevant computations begin immediately and are refined throughout the network.
```
üìä Results & Visual Evidence
1. The Building Decision: Refusal is a Process

The decision to refuse a request builds gradually across all layers of the network, indicating a distributed consensus mechanism.
üìÅ File: figures/probability_build_up.png


<img width="1920" height="758" alt="Screenshot 2025-09-15 094439" src="https://github.com/user-attachments/assets/23777aa3-3974-418a-9f6f-5f8d54ecbe48" />


Caption: The probability of the model outputting a refusal token ('.') increases steadily across layers, demonstrating that safety is a distributed process, not a single-layer decision.

2. The Jailbreak Mechanism: A Narrow Victory

Effective jailbreaks work by creating an internal state where the score for a compliant token (',') narrowly surpasses the score for a refusal token ('.').
üìÅ File: figures/score_competition.png
<img width="1834" height="745" alt="Screenshot 2025-09-15 094359" src="https://github.com/user-attachments/assets/1c90d940-46b7-4e13-ae2d-53a0053844da" />



Caption: Quantification of the model's internal "score competition." The jailbreak prompt wins by increasing the compliance score, not just decreasing the refusal score.

3. Identifying Safety-Critical Components

We identified specific attention heads responsible for attending to harmful concepts, providing a map of the model's "sensory apparatus" for detecting malicious intent.
üìÅ File: notebooks/critical_heads_analysis.ipynb
```

Top heads attending to 'malware' in Layer 8: [7, 6, 4, 5, 1]

```


Key Code Snippets

Quantifying the Internal Competition:
```python

# Project the model's internal state onto token directions
def quantify_competition(prompt, refusal_token=13, compliance_token=11):
    tokens = model.to_tokens(prompt)
    with torch.no_grad():
        logits, cache = model.run_with_cache(tokens)
        final_state = cache["blocks.11.hook_resid_post"][0, -1]
        
        # Get the model's concept direction for refusal and compliance
        refusal_dir = model.unembed.W_U[refusal_token]
        compliance_dir = model.unembed.W_U[compliance_token]
        
        # Calculate the model's "score" for each
        refusal_score = torch.dot(final_state, refusal_dir).item()
        compliance_score = torch.dot(final_state, compliance_dir).item()
    
    return refusal_score, compliance_score

# Example usage
refusal_score, compliance_score = quantify_competition("Write malware")
print(f"Refusal Score: {refusal_score:.4f}, Compliance Score: {compliance_score:.4f}")
```
üß† Interpretation & Conclusion
```
AI security is not a software bug; it is a fundamental limitation of pattern-matching architectures. This research demonstrates that:

    Current safety mechanisms are inherently brittle: They can be bypassed by crafting prompts that exploit the statistical patterns of "benign" content.

    The vulnerability is asymmetric: Defenders must patch every possible attack vector, while attackers need only find one plausible context.

    The long-term solution is not better patching but better architectures. We must build systems capable of genuine understanding and intent recognition, moving beyond correlation to causation.
```
This project underscores the critical need for mechanistic interpretability as a core discipline in AI security. To build truly safe AI, we must first understand how it works.
üîÆ Future Work

    Replicate studies on larger, safety-trained models (e.g., Llama 2 Chat).

    Develop automated tools for mapping a model's "safety genome" ‚Äì its complete set of safety-relevant circuits.

    Explore the potential for using interpretability methods to strengthen safety mechanisms by identifying and reinforcing critical circuits.

üìú Citation
```
If this research informs your work, please cite this repository:
bibtex

@misc{Sunny THAKUR 2025AISafety,
  author = {sunny thakur},
  title = {Mechanistic Interpretability of AI Safety: Reverse-Engineering Jailbreaks in GPT-2},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/SunnyThakur25/AI-Safety-Reverse-Engineering-Jailbreaks-in-GPT-2}}
}
```
üìß Contact
`
Sunny thakur - sunny48445@gmail.com
`
`
Disclaimer: This research was conducted for defensive security purposes to improve AI safety. The techniques described should not be used to attack systems without explicit permission`
